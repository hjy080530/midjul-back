from fastapi import APIRouter, File, UploadFile, Form, HTTPException

from app.models.schema import ProcessResponse, DifficultyWord, KeywordItem
from app.services.text_extractor import TextExtractor
from app.services.keyword_extractor import KeywordExtractor
from app.services.summarizer import Summarizer
from app.services.difficulty_analyzer import DifficultyAnalyzer
from app.config import get_settings
import time
import os
import aiofiles
import uuid
from datetime import datetime

router = APIRouter(prefix="/process", tags=["processing"])
settings = get_settings()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
text_extractor = TextExtractor()
keyword_extractor = KeywordExtractor(settings.KEYWORD_MODEL)
summarizer = Summarizer(settings.SUMMARY_MODEL)
difficulty_analyzer = DifficultyAnalyzer()

# ì„ì‹œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
documents_store = {}

from app.config import get_settings

settings = get_settings()


@router.post("/text", response_model=ProcessResponse)
async def process_text(text: str = Form(...)):
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ - ì¸ì¦ ì—†ìŒ"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ“¥ í…ìŠ¤íŠ¸ ë°›ìŒ: {len(text)}ì")
    print(f"{'=' * 50}\n")

    start_time = time.time()

    cleaned_text = text_extractor.clean_text(text)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords_raw = keyword_extractor.extract(cleaned_text)

    # í‚¤ì›Œë“œ ëœ»í’€ì´ ì¡°íšŒ (êµ­ë¦½êµ­ì–´ì› API)
    keyword_words = [word for word, score in keywords_raw]
    definitions = await keyword_extractor.get_definitions(
        keyword_words,
        settings.KOREAN_DICT_API_KEY
    )

    keywords = [
        KeywordItem(
            word=word,
            score=score,
            importance=keyword_extractor.categorize_importance(score),
            definition=definitions.get(word)
        )
        for word, score in keywords_raw
    ]

    # í•˜ì´ë¼ì´íŒ…
    highlighted = keyword_extractor.highlight_text_with_definitions(
        cleaned_text, keywords
    )

    # ìš”ì•½
    summary = summarizer.summarize(cleaned_text)

    # ë‚œì´ë„ ë¶„ì„
    difficult_words_raw = difficulty_analyzer.analyze_difficulty(cleaned_text)
    difficult_words = [DifficultyWord(**word) for word in difficult_words_raw]

    processing_time = time.time() - start_time

    document_id = str(uuid.uuid4())
    created_at = datetime.now().isoformat()

    response = ProcessResponse(
        id=document_id,
        original_text=cleaned_text,
        highlighted_html=highlighted['html'],
        highlighted_markdown=highlighted['markdown'],
        keywords=keywords,
        difficult_words=difficult_words,
        summary=summary,
        processing_time=processing_time,
        created_at=created_at
    )

    documents_store[document_id] = {
        "id": document_id,
        "original_text": cleaned_text,
        "highlighted_html": highlighted['html'],
        "highlighted_markdown": highlighted['markdown'],
        "keywords": [kw.dict() for kw in keywords],
        "difficult_words": [dw.dict() for dw in difficult_words],
        "summary": summary,
        "processing_time": processing_time,
        "created_at": created_at
    }

    print(f"âœ… ë¬¸ì„œ ì €ì¥ë¨: {document_id}")
    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n")

    return response

@router.get("/documents/{document_id}")
async def get_document(document_id: str):
    """ë¬¸ì„œ ì¡°íšŒ - ì¸ì¦ ì—†ìŒ"""
    print(f"ğŸ“– ë¬¸ì„œ ì¡°íšŒ: {document_id}")

    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    doc = documents_store[document_id]

    return ProcessResponse(
        id=doc["id"],
        original_text=doc["original_text"],
        highlighted_html=doc["highlighted_html"],
        highlighted_markdown=doc["highlighted_markdown"],
        keywords=[KeywordItem(**kw) for kw in doc["keywords"]],
        difficult_words=[DifficultyWord(**dw) for dw in doc["difficult_words"]],
        summary=doc["summary"],
        processing_time=doc["processing_time"],
        created_at=doc["created_at"]
    )


@router.get("/documents")
async def get_documents():
    """ë¬¸ì„œ ëª©ë¡ - ì¸ì¦ ì—†ìŒ"""
    return [
        {
            "id": doc["id"],
            "title": doc["original_text"][:50] + "...",
            "summary": doc["summary"],
            "created_at": doc["created_at"]
        }
        for doc in documents_store.values()
    ]


@router.delete("/documents/{document_id}")
async def delete_document(document_id: str):
    """ë¬¸ì„œ ì‚­ì œ - ì¸ì¦ ì—†ìŒ"""
    if document_id not in documents_store:
        raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    del documents_store[document_id]
    print(f"ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œë¨: {document_id}")

    return {"message": "ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}


@router.post("/pdf", response_model=ProcessResponse)
async def process_pdf(file: UploadFile = File(...)):
    """PDF íŒŒì¼ ì²˜ë¦¬ - ì¸ì¦ ì—†ìŒ"""
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

    file_path = f"{settings.UPLOAD_DIR}/{uuid.uuid4()}_{file.filename}"

    async with aiofiles.open(file_path, 'wb') as f:
        content = await file.read()
        await f.write(content)

    try:
        print(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {file.filename}")

        text = text_extractor.extract_from_pdf(file_path)

        start_time = time.time()
        cleaned_text = text_extractor.clean_text(text)
        keywords_raw = keyword_extractor.extract(cleaned_text)
        keywords = [
            KeywordItem(word=word, score=score, importance=keyword_extractor.categorize_importance(score))
            for word, score in keywords_raw
        ]
        highlighted = keyword_extractor.highlight_text(cleaned_text, keywords_raw)
        summary = summarizer.summarize(cleaned_text)
        difficult_words_raw = difficulty_analyzer.analyze_difficulty(cleaned_text)
        difficult_words = [DifficultyWord(**word) for word in difficult_words_raw]
        processing_time = time.time() - start_time

        document_id = str(uuid.uuid4())
        created_at = datetime.now().isoformat()

        response = ProcessResponse(
            id=document_id,
            original_text=cleaned_text,
            highlighted_html=highlighted['html'],
            highlighted_markdown=highlighted['markdown'],
            keywords=keywords,
            difficult_words=difficult_words,
            summary=summary,
            processing_time=processing_time,
            created_at=created_at
        )

        documents_store[document_id] = {
            "id": document_id,
            "original_text": cleaned_text,
            "highlighted_html": highlighted['html'],
            "highlighted_markdown": highlighted['markdown'],
            "keywords": [kw.dict() for kw in keywords],
            "difficult_words": [dw.dict() for dw in difficult_words],
            "summary": summary,
            "processing_time": processing_time,
            "created_at": created_at
        }

        print(f"âœ… PDF ë¬¸ì„œ ì €ì¥ë¨: {document_id}")

        return response

    finally:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {file.filename}")