from fastapi import APIRouter, File, UploadFile, Form, HTTPException, Depends

from app.models.schema import ProcessResponse, KeywordItem, DifficultyWord
from app.services.supabase_client import get_supabase
from app.services.text_extractor import TextExtractor
from app.services.keyword_extractor import KeywordExtractor
from app.services.summarizer import Summarizer
from app.services.difficulty_analyzer import DifficultyAnalyzer
from app.core.dependencies import get_current_user_id
from app.config import get_settings
from supabase import Client
import time
import os
import aiofiles
import uuid
from datetime import datetime

router = APIRouter(prefix="/process", tags=["processing"])
settings = get_settings()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
text_extractor = TextExtractor()
keyword_extractor = KeywordExtractor(settings.KEYWORD_MODEL)
summarizer = Summarizer(settings.SUMMARY_MODEL)
difficulty_analyzer = DifficultyAnalyzer()


@router.post("/text", response_model=ProcessResponse)
async def process_text(
        text: str = Form(...),
        user_id: str = Depends(get_current_user_id)
):
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ + DB ì €ì¥"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ“¥ í…ìŠ¤íŠ¸ ë°›ìŒ: {len(text)}ì")
    print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_id}")
    print(f"{'=' * 50}\n")

    start_time = time.time()

    # 1. í…ìŠ¤íŠ¸ ì •ì œ
    cleaned_text = text_extractor.clean_text(text)

    # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords_raw = keyword_extractor.extract(cleaned_text)

    # 3. êµ­ë¦½êµ­ì–´ì› ëœ»í’€ì´
    keyword_words = [word for word, score in keywords_raw]
    definitions = await keyword_extractor.get_definitions(
        keyword_words,
        settings.KOREAN_DICT_API_KEY
    )

    # âœ… ì „ì²´ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    all_scores = [score for word, score in keywords_raw]

    keywords = [
        KeywordItem(
            word=word,
            score=score,
            importance=keyword_extractor.categorize_importance(score, all_scores),  # âœ… all_scores ì¶”ê°€
            definition=definitions.get(word)
        )
        for word, score in keywords_raw
    ]

    # 4. í•˜ì´ë¼ì´íŒ…
    highlighted = keyword_extractor.highlight_text_with_definitions(
        cleaned_text, keywords
    )

    # 5. ìš”ì•½
    summary = summarizer.summarize(cleaned_text)

    # 6. ë‚œì´ë„ ë¶„ì„
    difficult_words_raw = difficulty_analyzer.analyze_difficulty(cleaned_text)
    difficult_words = [DifficultyWord(**word) for word in difficult_words_raw]

    processing_time = time.time() - start_time

    # 7. DB ì €ì¥
    document_id = None
    created_at = datetime.now().isoformat()

    try:
        supabase = get_supabase()

        if supabase:
            document_data = {
                "user_id": user_id,
                "title": cleaned_text[:100] + ("..." if len(cleaned_text) > 100 else ""),
                "original_text": cleaned_text,
                "highlighted_html": highlighted['html'],
                "highlighted_markdown": highlighted['markdown'],
                "keywords": [kw.dict() for kw in keywords],
                "difficult_words": [dw.dict() for dw in difficult_words],
                "summary": summary,
                "processing_time": processing_time
            }

            result = supabase.table("documents").insert(document_data).execute()

            if result.data and len(result.data) > 0:
                saved_doc = result.data[0]
                document_id = saved_doc["id"]
                created_at = saved_doc["created_at"]
                print(f"âœ… DB ì €ì¥ ì„±ê³µ: {document_id}")
            else:
                print(f"âš ï¸ DB ì €ì¥ ê²°ê³¼ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

    # 8. ì‘ë‹µ (DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜)
    if not document_id:
        document_id = str(uuid.uuid4())
        print(f"âš ï¸ ì„ì‹œ ID ì‚¬ìš©: {document_id}")

    response = ProcessResponse(
        id=document_id,
        original_text=cleaned_text,
        highlighted_html=highlighted['html'],
        highlighted_markdown=highlighted['markdown'],
        keywords=keywords,
        difficult_words=difficult_words,
        summary=summary,
        processing_time=processing_time,
        created_at=created_at
    )

    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n")

    return response


@router.post("/pdf", response_model=ProcessResponse)
async def process_pdf(
        file: UploadFile = File(...),
        user_id: str = Depends(get_current_user_id)
):
    """PDF íŒŒì¼ ì²˜ë¦¬ + DB ì €ì¥"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ“¥ PDF íŒŒì¼ ë°›ìŒ: {file.filename}")
    print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_id}")
    print(f"{'=' * 50}\n")

    start_time = time.time()

    # íŒŒì¼ í™•ì¥ì ê²€ì¦
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

    # ì„ì‹œ íŒŒì¼ ì €ì¥
    temp_file_path = f"/tmp/{uuid.uuid4()}_{file.filename}"

    try:
        async with aiofiles.open(temp_file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)

        # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = text_extractor.extract_from_pdf(temp_file_path)

        if not extracted_text or len(extracted_text.strip()) < 10:
            raise HTTPException(status_code=400, detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # 2. í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_text = text_extractor.clean_text(extracted_text)

        # 3. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_raw = keyword_extractor.extract(cleaned_text)

        # 4. êµ­ë¦½êµ­ì–´ì› ëœ»í’€ì´
        keyword_words = [word for word, score in keywords_raw]
        definitions = await keyword_extractor.get_definitions(
            keyword_words,
            settings.KOREAN_DICT_API_KEY
        )

        # âœ… ì „ì²´ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        all_scores = [score for word, score in keywords_raw]

        keywords = [
            KeywordItem(
                word=word,
                score=score,
                importance=keyword_extractor.categorize_importance(score, all_scores),  # âœ… all_scores ì¶”ê°€
                definition=definitions.get(word)
            )
            for word, score in keywords_raw
        ]

        # 5. í•˜ì´ë¼ì´íŒ…
        highlighted = keyword_extractor.highlight_text_with_definitions(
            cleaned_text, keywords
        )

        # 6. ìš”ì•½
        summary = summarizer.summarize(cleaned_text)

        # 7. ë‚œì´ë„ ë¶„ì„
        difficult_words_raw = difficulty_analyzer.analyze_difficulty(cleaned_text)
        difficult_words = [DifficultyWord(**word) for word in difficult_words_raw]

        processing_time = time.time() - start_time

        # 8. DB ì €ì¥
        document_id = None
        created_at = datetime.now().isoformat()

        try:
            supabase = get_supabase()

            if supabase:
                document_data = {
                    "user_id": user_id,
                    "title": file.filename,
                    "original_text": cleaned_text,
                    "highlighted_html": highlighted['html'],
                    "highlighted_markdown": highlighted['markdown'],
                    "keywords": [kw.dict() for kw in keywords],
                    "difficult_words": [dw.dict() for dw in difficult_words],
                    "summary": summary,
                    "processing_time": processing_time
                }

                result = supabase.table("documents").insert(document_data).execute()

                if result.data and len(result.data) > 0:
                    saved_doc = result.data[0]
                    document_id = saved_doc["id"]
                    created_at = saved_doc["created_at"]
                    print(f"âœ… DB ì €ì¥ ì„±ê³µ: {document_id}")
                else:
                    print(f"âš ï¸ DB ì €ì¥ ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

        # 9. ì‘ë‹µ (DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜)
        if not document_id:
            document_id = str(uuid.uuid4())
            print(f"âš ï¸ ì„ì‹œ ID ì‚¬ìš©: {document_id}")

        response = ProcessResponse(
            id=document_id,
            original_text=cleaned_text,
            highlighted_html=highlighted['html'],
            highlighted_markdown=highlighted['markdown'],
            keywords=keywords,
            difficult_words=difficult_words,
            summary=summary,
            processing_time=processing_time,
            created_at=created_at
        )

        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n")

        return response

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
            print(f"ğŸ—‘ï¸  ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_file_path}")


@router.get("/documents/{document_id}")
async def get_document(
        document_id: str,
        user_id: str = Depends(get_current_user_id)
):
    """ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ"""
    try:
        supabase = get_supabase()
        if not supabase:
            raise HTTPException(status_code=503, detail="DB ì—°ê²° ë¶ˆê°€")

        result = supabase.table("documents") \
            .select("*") \
            .eq("id", document_id) \
            .eq("user_id", user_id) \
            .single() \
            .execute()

        if not result.data:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        doc = result.data

        return ProcessResponse(
            id=doc["id"],
            original_text=doc["original_text"],
            highlighted_html=doc["highlighted_html"],
            highlighted_markdown=doc["highlighted_markdown"],
            keywords=[KeywordItem(**kw) for kw in doc["keywords"]],
            difficult_words=[DifficultyWord(**dw) for dw in doc["difficult_words"]],
            summary=doc["summary"],
            processing_time=doc["processing_time"],
            created_at=doc["created_at"]
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨")


@router.get("/documents")
async def get_documents(
        user_id: str = Depends(get_current_user_id),
        limit: int = 20,
        offset: int = 0
):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        supabase = get_supabase()
        if not supabase:
            return []

        result = supabase.table("documents") \
            .select("id, title, summary, created_at") \
            .eq("user_id", user_id) \
            .order("created_at", desc=True) \
            .range(offset, offset + limit - 1) \
            .execute()

        return result.data
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


@router.delete("/documents/{document_id}")
async def delete_document(
        document_id: str,
        user_id: str = Depends(get_current_user_id)
):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        supabase = get_supabase()
        if not supabase:
            raise HTTPException(status_code=503, detail="DB ì—°ê²° ë¶ˆê°€")

        result = supabase.table("documents") \
            .delete() \
            .eq("id", document_id) \
            .eq("user_id", user_id) \
            .execute()

        return {"message": "ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨")