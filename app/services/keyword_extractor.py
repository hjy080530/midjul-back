from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from collections import Counter
import httpx
import xml.etree.ElementTree as ET


class KeywordExtractor:
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.kw_model = KeyBERT(self.model)

        # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°
        try:
            from kiwipiepy import Kiwi
            self.kiwi = Kiwi()
            print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Kiwi ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.kiwi = None

        print("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def _stopwords(self) -> set:
        """ë¶ˆìš©ì–´ ëª©ë¡"""
        return {
            'ê²ƒ', 'ì´', 'ê·¸', 'ì €', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ìˆ˜', 'ë°', 'ë°”',
            'ì§€', 'ë“±', 'ë•Œ', 'ê³³', 'ì ', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ',
            'ê°œ', 'ëª…', 'ê¶Œ', 'ì¥', 'ë•Œë¬¸', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ëŒ', 'ì‚¬ëŒë“¤',
            'ìš°ë¦¬', 'ì €í¬', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì–´ë–¤', 'ëª¨ë“ ', 'ê°', 'ëª‡',
            'í•˜ê¸°', 'ë˜ê¸°', 'ìˆê¸°', 'ì—†ê¸°', 'ê°™ê¸°', 'ì•ˆ', 'ëª»', 'ë”', 'ëœ',
            'ì¢€', 'ì•½ê°„', 'ì¡°ê¸ˆ', 'ë§ì´', 'ì˜ˆë¥¼', 'ì˜ˆì‹œ', 'ì˜ˆ', 'ë“¤ì–´',
            'í†µí•´', 'ìœ„í•´', 'ëŒ€í•´', 'ê´€í•´'
        }

    def extract(self, text: str, top_n: int = 15) -> List[Tuple[str, float]]:
        """Kiwi + KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""

        print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")

        if not self.kiwi:
            print("âš ï¸ Kiwi ì—†ìŒ")
            return self._fallback_keywords(text, top_n)

        # 1. Kiwië¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
        nouns = self._extract_nouns_only(text)
        print(f"âœ… ëª…ì‚¬ ì¶”ì¶œ: {len(set(nouns))}ê°œ")

        if len(nouns) == 0:
            return []

        # 2. ë¹ˆë„ ê³„ì‚°
        noun_freq = Counter(nouns)

        # 3. ì˜ë¯¸ìˆëŠ” ëª…ì‚¬ë§Œ í•„í„°ë§
        meaningful_nouns = []
        for noun, freq in noun_freq.items():
            if (len(noun) >= 3) or (len(noun) >= 2 and freq >= 2):
                if noun not in self._stopwords():
                    meaningful_nouns.append(noun)

        print(f"âœ… ì˜ë¯¸ìˆëŠ” ëª…ì‚¬: {len(meaningful_nouns)}ê°œ")

        if len(meaningful_nouns) == 0:
            return []

        # 4. KeyBERTë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        try:
            keywords = self.kw_model.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 2),
                stop_words=list(self._stopwords()),
                top_n=min(top_n * 2, len(meaningful_nouns)),
                candidates=meaningful_nouns,
                use_maxsum=True,
                diversity=0.7
            )
            print(f"âœ… KeyBERT ì¶”ì¶œ: {len(keywords)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ KeyBERT ì‹¤íŒ¨: {e}")
            total = sum(noun_freq.values())
            keywords = [
                (noun, freq / total)
                for noun, freq in noun_freq.most_common(top_n)
                if noun in meaningful_nouns
            ]

        final = keywords[:top_n]
        print(f"âœ… ìµœì¢… í‚¤ì›Œë“œ: {len(final)}ê°œ")
        for word, score in final[:5]:
            print(f"  - {word}: {score:.3f}")

        return final

    def _extract_nouns_only(self, text: str) -> List[str]:
        """Kiwië¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ"""
        try:
            result = self.kiwi.analyze(text)
            nouns = []

            for token in result[0][0]:
                if token.tag in ['NNG', 'NNP']:
                    word = token.form
                    if (len(word) >= 2 and
                            word not in self._stopwords() and
                            re.match(r'^[ê°€-í£]+$', word)):
                        nouns.append(word)

            return nouns
        except Exception as e:
            print(f"âŒ Kiwi ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def _fallback_keywords(self, text: str, top_n: int) -> List[Tuple[str, float]]:
        """í´ë°±: ì •ê·œì‹ ê¸°ë°˜"""
        words = re.findall(r'[ê°€-í£]{2,}', text)
        words = [w for w in words if w not in self._stopwords()]

        freq = Counter(words)
        total = sum(freq.values()) if freq else 1

        return [(word, count / total) for word, count in freq.most_common(top_n)]

    async def get_definitions(self, keywords: List[str], api_key: str) -> Dict[str, str]:
        """êµ­ë¦½êµ­ì–´ì› APIë¡œ ëœ» ê°€ì ¸ì˜¤ê¸° (XML ì‘ë‹µ)"""
        definitions = {}

        print(f"\nğŸ“š êµ­ë¦½êµ­ì–´ì› ì‚¬ì „ ì¡°íšŒ ì‹œì‘: {len(keywords)}ê°œ")

        for word in keywords:
            if not re.match(r'^[ê°€-í£]+$', word) or len(word) < 2:
                continue

            definition = await self._fetch_definition_from_api(word, api_key)

            if definition:
                definitions[word] = definition
                print(f"  âœ… {word}: {definition[:40]}...")
            else:
                definitions[word] = f"'{word}'ì— ëŒ€í•œ ì„¤ëª…"
                print(f"  âš ï¸ {word}: ì‚¬ì „ì— ì—†ìŒ")

        print(f"ğŸ“š ì‚¬ì „ ì¡°íšŒ ì™„ë£Œ: {len(definitions)}ê°œ\n")

        return definitions

    async def _fetch_definition_from_api(self, word: str, api_key: str) -> str:
        """êµ­ë¦½êµ­ì–´ì› API í˜¸ì¶œ"""
        try:
            url = "https://stdict.korean.go.kr/api/search.do"

            params = {
                "key": api_key,
                "type_search": "search",
                "q": word,
                "req_type": "xml",
                "start": "1",
                "num": "10",
            }

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url, params=params)

                if response.status_code != 200:
                    return None

                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                content = response.content.decode('utf-8')

                # XML íŒŒì‹±
                try:
                    root = ET.fromstring(response.content)
                except Exception as e:
                    print(f"    âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
                    return None

                # error íƒœê·¸ê°€ ë£¨íŠ¸ì¸ ê²½ìš°
                if root.tag == 'error':
                    error_message = root.text
                    print(f"    âŒ API ì—ëŸ¬: {error_message}")
                    print(f"    ğŸ“„ ì „ì²´ ì‘ë‹µ:\n{content}\n")
                    return None

                # channelì´ ì•„ë‹Œ ê²½ìš°
                if root.tag != 'channel':
                    print(f"    âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë£¨íŠ¸: {root.tag}")
                    print(f"    ğŸ“„ ì „ì²´ ì‘ë‹µ:\n{content}\n")
                    return None

                # total í™•ì¸
                total_elem = root.find('.//total')
                if total_elem is not None:
                    total = total_elem.text
                    if total == '0':
                        return None

                # ì²« ë²ˆì§¸ item
                item = root.find('.//item')
                if item is None:
                    return None

                # sense > definition
                sense = item.find('.//sense')
                if sense is None:
                    return None

                definition_elem = sense.find('.//definition')
                if definition_elem is None or not definition_elem.text:
                    return None

                definition = definition_elem.text.strip()

                # 80ì ì œí•œ
                if len(definition) > 80:
                    definition = definition[:80] + "..."

                return definition

        except Exception as e:
            print(f"    âŒ ì˜ˆì™¸: {e}")
            return None

    def categorize_importance(self, score: float) -> str:
        """ì¤‘ìš”ë„ ë¶„ë¥˜"""
        if score >= 0.4:
            return "high"
        elif score >= 0.2:
            return "medium"
        return "low"

    def highlight_text_with_definitions(self, text: str, keywords: List) -> dict:
        """í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… - ì¡°ì‚¬ í¬í•¨"""
        import html as html_module

        original_text = text
        sorted_keywords = sorted(keywords, key=lambda x: len(x.word), reverse=True)

        highlights = []

        print(f"ğŸ¨ í•˜ì´ë¼ì´íŒ… ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ")

        for kw in sorted_keywords:
            word = kw.word

            if word not in original_text:
                continue

            # ë‹¨ì–´ ë’¤ì— ì¡°ì‚¬/ë¬¸ì¥ë¶€í˜¸ê°€ ì™€ë„ ë§¤ì¹­
            # ì•: í•œê¸€ì´ ì•„ë‹Œ ê²ƒ, ë’¤: í•œê¸€ ë˜ëŠ” ì¡°ì‚¬ ì‹œì‘ ê¸€ì
            pattern = re.compile(
                f'(?<![ê°€-í£])({re.escape(word)})(?=[^ê°€-í£]|[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ê²Œì˜ë„ë§Œë¶€í„°ê¹Œì§€]|$)'
            )

            matches = list(pattern.finditer(original_text))

            if len(matches) > 0:
                print(f"  âœ… '{word}': {len(matches)}ê°œ ë§¤ì¹­")

            for match in matches:
                start, end = match.span(1)  # ê·¸ë£¹ 1ë§Œ (ë‹¨ì–´ë§Œ)

                # ê²¹ì¹¨ í™•ì¸
                overlap = False
                for h_start, h_end, _, _, _, _ in highlights:
                    if (start < h_end and end > h_start):
                        overlap = True
                        break

                if not overlap:
                    highlights.append((
                        start, end, word,
                        kw.definition or f"'{word}'",
                        kw.importance,
                        kw.score
                    ))

        highlights.sort(key=lambda x: x[0])

        print(f"\nâœ… ì´ {len(highlights)}ê°œ í•˜ì´ë¼ì´íŠ¸\n")

        # HTML ìƒì„±
        html_parts = []
        last_end = 0

        colors = {
            "high": "#FFD700",
            "medium": "#87CEEB",
            "low": "#E8E8E8"
        }

        for start, end, word, definition, importance, score in highlights:
            html_parts.append(original_text[last_end:start])

            word_escaped = html_module.escape(word, quote=True)
            definition_escaped = html_module.escape(definition, quote=True)
            color = colors[importance]

            html_parts.append(
                f'<mark class="highlight-{importance} keyword-tooltip" '
                f'style="background-color:{color}; padding:2px 4px; border-radius:3px; cursor:help;" '
                f'data-word="{word_escaped}" '
                f'data-definition="{definition_escaped}" '
                f'data-score="{score:.2f}">'
                f'{original_text[start:end]}'
                f'</mark>'
            )

            last_end = end

        html_parts.append(original_text[last_end:])

        return {
            "html": ''.join(html_parts),
            "markdown": original_text
        }